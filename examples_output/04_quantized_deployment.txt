================================================================================
Quantized Model Deployment Examples
================================================================================

================================================================================
üìä Quantization Scheme Selection Guide
================================================================================

Scheme               Compression     GPU Needs                 Best For                      
-------------------- --------------- ------------------------- ------------------------------
Baseline (FP16)      1.0x             Any GPU                   Development, maximum accuracy required
FP8                  2.0x             L4, A100, H100 (Ampere+)  Production deployment, good accuracy
W4A16 (AWQ)          3.7x             Any GPU (CPU decompression) Maximum cost savings, most popular

================================================================================
üí° Recommendations:
================================================================================
‚Ä¢ Start with FP8 - easiest to deploy, 2x cost savings
‚Ä¢ Use W4A16 for maximum savings - 3.7x compression
‚Ä¢ Baseline only for development or accuracy validation

‚ö†Ô∏è  Note: W4A16 requires pre-quantized model from HuggingFace
   Example: TheBloke/Phi-2-AWQ or similar quantized variants


‚ö†Ô∏è  Note: These examples load models and may take 1-2 minutes each
Press Ctrl+C to skip to next example


================================================================================
Example 1: Baseline FP16 Deployment
================================================================================

Loading microsoft/phi-2 in FP16 (no quantization)...
INFO 11-02 04:23:09 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='microsoft/phi-2', speculative_config=None, tokenizer='microsoft/phi-2', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=microsoft/phi-2, use_v2_block_manager=False, enable_prefix_caching=False)
INFO 11-02 04:23:09 selector.py:184] Cannot use FlashAttention-2 backend for head size 80.
INFO 11-02 04:23:09 selector.py:54] Using XFormers backend.
INFO 11-02 04:23:11 model_runner.py:720] Starting to load model microsoft/phi-2...
INFO 11-02 04:23:11 selector.py:184] Cannot use FlashAttention-2 backend for head size 80.
INFO 11-02 04:23:11 selector.py:54] Using XFormers backend.
INFO 11-02 04:23:11 weight_utils.py:225] Using model weights format ['*.safetensors']
INFO 11-02 04:23:13 model_runner.py:732] Loading model weights took 5.1933 GB
INFO 11-02 04:23:14 gpu_executor.py:102] # GPU blocks: 2881, # CPU blocks: 819
INFO 11-02 04:23:16 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 11-02 04:23:16 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 11-02 04:23:34 model_runner.py:1225] Graph capturing finished in 18 secs.
‚úÖ Generated 100 tokens in 2.38s
‚úÖ Throughput: 42.0 tokens/sec

üìù Response:


Solution:
Quantization is a technique used in machine learning to improve the performance of certain models. It involves reducing the precision of numerical values used in calculations, such as the weights and biases in the model. By quantizing these values, unnecessary computational overhead is reduced, leading to faster and more efficient inference. Quantization also helps in reducing memory usage and can improve the model's stability and robustness.

Follow-up exercises:
1. How does quantization improve the performance


================================================================================
Example 2: FP8 Quantized Deployment (2x compression)
================================================================================

Loading microsoft/phi-2 with FP8 quantization...
üíæ Memory savings: 2x compression (50% reduction)
‚ö° Expected speedup: 2-3x faster inference
INFO 11-02 04:23:36 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='microsoft/phi-2', speculative_config=None, tokenizer='microsoft/phi-2', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=microsoft/phi-2, use_v2_block_manager=False, enable_prefix_caching=False)
INFO 11-02 04:23:37 model_runner.py:720] Starting to load model microsoft/phi-2...
‚ùå FP8 quantization failed: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 22.03 GiB of which 43.06 MiB is free. Including non-PyTorch memory, this process has 21.98 GiB memory in use. Of the allocated memory 21.44 GiB is allocated by PyTorch, with 37.34 MiB allocated in private pools (e.g., CUDA Graphs), and 153.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
‚ö†Ô∏è  FP8 requires Ampere+ GPUs (L4, A100, H100)
   Your GPU may not support FP8 quantization

================================================================================
Example 3: W4A16 (AWQ) Deployment (3.7x compression)
================================================================================

‚ö†Ô∏è  AWQ requires pre-quantized model from HuggingFace
   Example: TheBloke/Mistral-7B-v0.1-AWQ

üìù Code example:


# Load pre-quantized AWQ model
llm = LLM(
    model="TheBloke/Mistral-7B-v0.1-AWQ",  # Pre-quantized model
    quantization="awq",                     # Enable AWQ
    trust_remote_code=True,
    gpu_memory_utilization=0.9,
)

# Generate as normal
outputs = llm.generate(prompts, sampling_params)

================================================================================
üí∞ Cost Analysis (Mistral-7B on L4 GPU)
================================================================================
Baseline FP16:  Requires A100 (26GB model)
                Cost: $26.40/day ($792/month)

W4A16 (AWQ):    Fits on L4! (7GB compressed)
                Cost: $10.80/day ($324/month)

üíµ Savings: $15.60/day ($468/month, $5,616/year)
‚ú® Bonus: Can use cheaper L4 GPU instead of A100!

================================================================================
Example 4: Production Deployment Pattern
================================================================================

üìù Recommended Production Setup:


import os
from vllm import LLM, SamplingParams

# Configuration from environment
MODEL_NAME = os.getenv("MODEL_NAME", "microsoft/phi-2")
QUANTIZATION = os.getenv("QUANTIZATION", "fp8")  # fp8, awq, none
GPU_MEMORY = float(os.getenv("GPU_MEMORY_UTILIZATION", "0.9"))

# Load model with quantization
llm = LLM(
    model=MODEL_NAME,
    quantization=QUANTIZATION if QUANTIZATION != "none" else None,
    trust_remote_code=True,
    gpu_memory_utilization=GPU_MEMORY,
    max_model_len=2048,
)

# Sampling config
sampling_params = SamplingParams(
    temperature=0.7,
    max_tokens=500,
    top_p=0.9,
)

# Generate
def generate(prompt: str) -> str:
    outputs = llm.generate([prompt], sampling_params)
    return outputs[0].outputs[0].text

# Use in production
response = generate("Your prompt here")


================================================================================
üê≥ Docker Environment Variables
================================================================================
MODEL_NAME=microsoft/phi-2
QUANTIZATION=fp8           # Options: fp8, awq, none
GPU_MEMORY_UTILIZATION=0.9

üí° Set QUANTIZATION=fp8 in .env file for 2x cost savings

================================================================================
‚úÖ Summary
================================================================================

1. FP8: Easiest to deploy, 2x compression, production-ready
2. W4A16 (AWQ): Best compression at 3.7x, requires pre-quantized model
3. Use environment variables for flexible configuration
4. Quantization is production standard - always use it!

üí∞ Cost Impact:
   Baseline (FP16):  $324/month
   FP8 Quantized:    $162/month  (50% savings)
   W4A16 Quantized:  $87/month   (73% savings)

üìö Next Steps:
   1. Test FP8 quantization on your model
   2. Find pre-quantized AWQ models on HuggingFace (TheBloke/*-AWQ)
   3. Update Docker .env with QUANTIZATION=fp8
   4. Deploy and enjoy 2-3.7x cost reduction!
