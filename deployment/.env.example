# vLLM Production Configuration
# Copy this to .env and customize for your deployment

# =============================================================================
# Model Configuration
# =============================================================================

# Model to serve (HuggingFace model name or local path)
MODEL_NAME=microsoft/phi-2

# GPU memory utilization (0.0 to 1.0)
# Lower values leave more VRAM for KV cache
GPU_MEMORY_UTILIZATION=0.9

# Maximum sequence length
MAX_MODEL_LEN=2048

# Trust remote code (required for some models like Phi-2)
TRUST_REMOTE_CODE=True

# =============================================================================
# Quantization (IMPORTANT: 2-3.7x cost reduction!)
# =============================================================================

# Quantization scheme: none, fp8, awq, gptq
# - none: No quantization (baseline FP16)
# - fp8: Float8 quantization, 2x compression, easiest (requires Ampere+ GPU)
# - awq: 4-bit weights, 3.7x compression, best savings (requires pre-quantized model)
# - gptq: 4-bit weights, 3.7x compression (requires pre-quantized model)
#
# Production recommendation: Start with fp8 for 2x savings
QUANTIZATION=fp8

# Data type: auto, half, float16, bfloat16
# Only needed if QUANTIZATION=none
DTYPE=auto

# =============================================================================
# Server Configuration
# =============================================================================

# Host and port
HOST=0.0.0.0
PORT=8000

# Number of workers (usually 1 for GPU workloads)
WORKERS=1

# Log level (debug, info, warning, error)
LOG_LEVEL=info

# =============================================================================
# Cost Budgeting (Optional)
# =============================================================================

# Daily cost budget in dollars
DAILY_BUDGET=50.00

# Cost per 1M input tokens (for tracking)
INPUT_COST_PER_1M=0.10

# Cost per 1M output tokens (for tracking)
OUTPUT_COST_PER_1M=0.10

# GPU rental cost per hour (for ROI calculations)
GPU_COST_PER_HOUR=0.45

# =============================================================================
# SLA Configuration (Optional)
# =============================================================================

# Maximum allowed latency in milliseconds
MAX_LATENCY_MS=1000

# Minimum uptime percentage
MIN_UPTIME_PCT=99.5

# =============================================================================
# Model Selection for Intelligent Routing (Optional)
# =============================================================================

# Available models (comma-separated)
# AVAILABLE_MODELS=microsoft/phi-2,mistralai/Mistral-7B-v0.1,meta-llama/Llama-2-13b-hf

# =============================================================================
# Security (Production)
# =============================================================================

# API key for authentication (optional)
# API_KEY=your-secret-api-key-here

# Allowed origins for CORS (comma-separated)
# CORS_ORIGINS=https://yourdomain.com,https://app.yourdomain.com

# =============================================================================
# Monitoring (Optional)
# =============================================================================

# Enable Prometheus metrics endpoint
ENABLE_METRICS=true

# Metrics port (if different from main port)
# METRICS_PORT=9090

# =============================================================================
# Advanced vLLM Options
# =============================================================================

# Tensor parallelism size (for multi-GPU)
# TENSOR_PARALLEL_SIZE=1

# Pipeline parallelism size
# PIPELINE_PARALLEL_SIZE=1

# Quantization method (none, awq, gptq)
# QUANTIZATION=none

# Enable prefix caching
# ENABLE_PREFIX_CACHING=False

# Swap space in GB (for CPU offloading)
# SWAP_SPACE=4

# =============================================================================
# Development / Testing
# =============================================================================

# Enable debug mode
# DEBUG=False

# Enable hot reload
# RELOAD=False
