#!/usr/bin/env python3
"""
Example 6: Prefix Caching for 50-80% Cost Reduction
Demonstrates vLLM prefix caching for repeated prompts

Use cases:
- RAG with fixed system prompts (50-80% savings)
- Template-based generation (60-70% savings)
- Multi-turn conversations (40-60% savings)
- Code generation with boilerplate (50-70% savings)

Based on Neural Magic talk: Prefix caching = major cost optimization
"""

import time
from typing import List, Dict
from dataclasses import dataclass

try:
    from vllm import LLM, SamplingParams
    VLLM_AVAILABLE = True
except ImportError:
    VLLM_AVAILABLE = False
    print("âš ï¸  vLLM not available")


@dataclass
class CachingMetrics:
    """Metrics for prefix caching"""
    scenario: str
    num_requests: int
    total_tokens: int
    cached_tokens: int
    cache_hit_rate: float
    time_without_cache_s: float
    time_with_cache_s: float
    speedup: float
    cost_without_cache: float
    cost_with_cache: float
    savings_pct: float


def simulate_rag_with_caching():
    """
    Scenario 1: RAG with Fixed System Prompt

    Pattern:
    - System prompt: 500 tokens (document context)
    - User queries: 20-50 tokens each
    - 100 requests

    Savings: 80% (system prompt cached across all requests)
    """
    print(f"\n{'='*80}")
    print(f"Scenario 1: RAG with Fixed System Prompt")
    print(f"{'='*80}\n")

    # Simulate metrics
    system_prompt_tokens = 500  # Large document context
    avg_query_tokens = 30
    num_requests = 100

    # Without caching: process full prompt every time
    total_tokens_no_cache = num_requests * (system_prompt_tokens + avg_query_tokens)
    time_per_request_no_cache = 0.5  # seconds
    total_time_no_cache = num_requests * time_per_request_no_cache

    # With caching: system prompt cached after first request
    cached_tokens = system_prompt_tokens * 99  # Cached for 99/100 requests
    total_tokens_with_cache = (system_prompt_tokens + avg_query_tokens) + (99 * avg_query_tokens)
    time_per_cached_request = 0.1  # Much faster
    total_time_with_cache = time_per_request_no_cache + (99 * time_per_cached_request)

    # Cost calculation ($0.10 per 1M tokens)
    cost_per_1m_tokens = 0.10
    cost_no_cache = (total_tokens_no_cache / 1_000_000) * cost_per_1m_tokens
    cost_with_cache = (total_tokens_with_cache / 1_000_000) * cost_per_1m_tokens

    savings = ((total_tokens_no_cache - total_tokens_with_cache) / total_tokens_no_cache) * 100
    speedup = total_time_no_cache / total_time_with_cache

    print(f"ðŸ“Š Metrics:")
    print(f"  System prompt: {system_prompt_tokens} tokens (fixed)")
    print(f"  Query size: {avg_query_tokens} tokens (variable)")
    print(f"  Requests: {num_requests}")
    print(f"\nWithout Caching:")
    print(f"  Total tokens: {total_tokens_no_cache:,}")
    print(f"  Total time: {total_time_no_cache:.1f}s")
    print(f"  Cost: ${cost_no_cache:.6f}")
    print(f"\nWith Prefix Caching:")
    print(f"  Total tokens: {total_tokens_with_cache:,}")
    print(f"  Cached tokens: {cached_tokens:,}")
    print(f"  Cache hit rate: 99%")
    print(f"  Total time: {total_time_with_cache:.1f}s")
    print(f"  Cost: ${cost_with_cache:.6f}")
    print(f"\nðŸ’° Savings:")
    print(f"  Token reduction: {savings:.1f}%")
    print(f"  Cost savings: ${cost_no_cache - cost_with_cache:.6f} ({savings:.1f}%)")
    print(f"  Speedup: {speedup:.1f}x faster")

    return CachingMetrics(
        scenario="RAG with Fixed System Prompt",
        num_requests=num_requests,
        total_tokens=total_tokens_with_cache,
        cached_tokens=cached_tokens,
        cache_hit_rate=0.99,
        time_without_cache_s=total_time_no_cache,
        time_with_cache_s=total_time_with_cache,
        speedup=speedup,
        cost_without_cache=cost_no_cache,
        cost_with_cache=cost_with_cache,
        savings_pct=savings,
    )


def simulate_template_generation():
    """
    Scenario 2: Template-Based Code Generation

    Pattern:
    - Code template: 300 tokens (imports, boilerplate)
    - Specific instructions: 50 tokens
    - 50 code generation requests

    Savings: 70% (template cached)
    """
    print(f"\n{'='*80}")
    print(f"Scenario 2: Template-Based Code Generation")
    print(f"{'='*80}\n")

    template_tokens = 300  # Fixed boilerplate
    instruction_tokens = 50
    num_requests = 50

    # Without caching
    total_tokens_no_cache = num_requests * (template_tokens + instruction_tokens)
    time_no_cache = num_requests * 0.4

    # With caching
    cached_tokens = template_tokens * 49
    total_tokens_with_cache = (template_tokens + instruction_tokens) + (49 * instruction_tokens)
    time_with_cache = 0.4 + (49 * 0.12)

    # Cost
    cost_per_1m = 0.10
    cost_no_cache = (total_tokens_no_cache / 1_000_000) * cost_per_1m
    cost_with_cache = (total_tokens_with_cache / 1_000_000) * cost_per_1m

    savings = ((total_tokens_no_cache - total_tokens_with_cache) / total_tokens_no_cache) * 100
    speedup = time_no_cache / time_with_cache

    print(f"ðŸ“Š Metrics:")
    print(f"  Template: {template_tokens} tokens (fixed)")
    print(f"  Instructions: {instruction_tokens} tokens (variable)")
    print(f"  Requests: {num_requests}")
    print(f"\nWithout Caching:")
    print(f"  Total tokens: {total_tokens_no_cache:,}")
    print(f"  Cost: ${cost_no_cache:.6f}")
    print(f"\nWith Prefix Caching:")
    print(f"  Total tokens: {total_tokens_with_cache:,}")
    print(f"  Savings: {savings:.1f}%")
    print(f"  Cost: ${cost_with_cache:.6f}")
    print(f"  Speedup: {speedup:.1f}x")

    return CachingMetrics(
        scenario="Template-Based Generation",
        num_requests=num_requests,
        total_tokens=total_tokens_with_cache,
        cached_tokens=cached_tokens,
        cache_hit_rate=0.98,
        time_without_cache_s=time_no_cache,
        time_with_cache_s=time_with_cache,
        speedup=speedup,
        cost_without_cache=cost_no_cache,
        cost_with_cache=cost_with_cache,
        savings_pct=savings,
    )


def simulate_multi_turn_conversation():
    """
    Scenario 3: Multi-Turn Conversations

    Pattern:
    - Conversation history: grows from 100 to 500 tokens
    - New message: 30 tokens
    - 10 turns

    Savings: 50% (previous history cached)
    """
    print(f"\n{'='*80}")
    print(f"Scenario 3: Multi-Turn Conversation")
    print(f"{'='*80}\n")

    turns = 10
    initial_context = 100
    tokens_per_turn = 50  # User + assistant
    new_message = 30

    # Without caching: reprocess entire history each turn
    total_tokens_no_cache = sum(
        initial_context + (i * tokens_per_turn) + new_message
        for i in range(turns)
    )

    # With caching: only process new tokens
    total_tokens_with_cache = sum(
        new_message if i > 0 else (initial_context + new_message)
        for i in range(turns)
    )

    cached_tokens = total_tokens_no_cache - total_tokens_with_cache

    cost_per_1m = 0.10
    cost_no_cache = (total_tokens_no_cache / 1_000_000) * cost_per_1m
    cost_with_cache = (total_tokens_with_cache / 1_000_000) * cost_per_1m

    savings = ((total_tokens_no_cache - total_tokens_with_cache) / total_tokens_no_cache) * 100

    print(f"ðŸ“Š Metrics:")
    print(f"  Conversation turns: {turns}")
    print(f"  Initial context: {initial_context} tokens")
    print(f"  New message: {new_message} tokens per turn")
    print(f"\nWithout Caching:")
    print(f"  Total tokens: {total_tokens_no_cache:,}")
    print(f"  Cost: ${cost_no_cache:.6f}")
    print(f"\nWith Prefix Caching:")
    print(f"  Total tokens: {total_tokens_with_cache:,}")
    print(f"  Cached tokens: {cached_tokens:,}")
    print(f"  Savings: {savings:.1f}%")
    print(f"  Cost: ${cost_with_cache:.6f}")

    return CachingMetrics(
        scenario="Multi-Turn Conversation",
        num_requests=turns,
        total_tokens=total_tokens_with_cache,
        cached_tokens=cached_tokens,
        cache_hit_rate=0.9,
        time_without_cache_s=turns * 0.5,
        time_with_cache_s=turns * 0.25,
        speedup=2.0,
        cost_without_cache=cost_no_cache,
        cost_with_cache=cost_with_cache,
        savings_pct=savings,
    )


def vllm_prefix_caching_code_example():
    """Show how to enable prefix caching in vLLM"""
    print(f"\n{'='*80}")
    print(f"How to Enable Prefix Caching in vLLM")
    print(f"{'='*80}\n")

    code_example = '''
from vllm import LLM, SamplingParams

# Initialize vLLM with prefix caching enabled
llm = LLM(
    model="microsoft/phi-2",
    trust_remote_code=True,
    enable_prefix_caching=True,  # Enable prefix caching!
    max_model_len=2048,
)

# Create sampling params
sampling_params = SamplingParams(temperature=0.7, max_tokens=200)

# Fixed system prompt (will be cached)
system_prompt = """You are a financial analyst assistant.
Your task is to analyze financial documents and provide insights.
Always cite sources and be factual.
"""

# Multiple user queries (system prompt is cached)
user_queries = [
    "What are the key risk factors?",
    "Summarize the revenue trends.",
    "Analyze the profit margins.",
]

# Generate responses (system prompt cached after first request!)
for query in user_queries:
    full_prompt = system_prompt + f"\\n\\nQuery: {query}"
    output = llm.generate([full_prompt], sampling_params)
    print(output[0].outputs[0].text)
    # 80% faster for queries 2-N because system_prompt is cached!
'''

    print(code_example)

    print(f"\n{'='*80}")
    print(f"ðŸ’¡ Key Points:")
    print(f"{'='*80}")
    print(f"1. Set enable_prefix_caching=True when initializing LLM")
    print(f"2. Keep fixed/repeated content at the START of prompts")
    print(f"3. Variable content goes at the END")
    print(f"4. Cache is automatic - no manual management needed")
    print(f"5. Works across all requests with same prefix")


def production_deployment_pattern():
    """Show production deployment with prefix caching"""
    print(f"\n{'='*80}")
    print(f"Production Deployment Pattern")
    print(f"{'='*80}\n")

    print("ðŸ“ Update .env configuration:\n")
    print("# Enable prefix caching (add to .env)")
    print("ENABLE_PREFIX_CACHING=True\n")

    print("ðŸ“ RAG Pipeline with Caching:\n")

    code_example = '''
# RAG with prefix caching
def rag_with_caching(documents: List[str], query: str) -> str:
    """
    RAG pipeline optimized with prefix caching

    documents: Fixed document context (cached)
    query: Variable user query (not cached)
    """

    # Build prompt: fixed context first, variable query last
    context = "\\n\\n".join(documents)  # Fixed (cached)
    prompt = f"""Context:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:"""

    # Generate (context is cached after first call!)
    output = llm.generate([prompt], sampling_params)
    return output[0].outputs[0].text

# Usage
documents = load_financial_documents()  # Fixed for multiple queries

# These queries will be 50-80% faster because documents are cached!
answer1 = rag_with_caching(documents, "What are the key risks?")
answer2 = rag_with_caching(documents, "Summarize revenue trends.")
answer3 = rag_with_caching(documents, "Analyze profit margins.")
'''

    print(code_example)


def roi_analysis():
    """Calculate ROI for prefix caching"""
    print(f"\n{'='*80}")
    print(f"ðŸ’° ROI Analysis at 10K Requests/Day")
    print(f"{'='*80}\n")

    scenarios = [
        ("RAG (fixed context)", 80),
        ("Code gen (templates)", 70),
        ("Conversations", 50),
    ]

    baseline_cost_per_month = 324  # From our benchmarks

    print(f"{'Scenario':<25} {'Savings':<15} {'Monthly Cost':<15} {'Yearly Savings':<15}")
    print(f"{'-'*25} {'-'*15} {'-'*15} {'-'*15}")

    for scenario, savings_pct in scenarios:
        cost_with_cache = baseline_cost_per_month * (1 - savings_pct / 100)
        monthly_savings = baseline_cost_per_month - cost_with_cache
        yearly_savings = monthly_savings * 12

        print(f"{scenario:<25} {savings_pct}%{'':<12} ${cost_with_cache:<14.2f} ${yearly_savings:<14.2f}")

    print(f"\nðŸ’¡ Prefix caching is FREE - just enable it!")
    print(f"   No additional infrastructure cost")
    print(f"   Automatic performance + cost optimization")


def main():
    """Run prefix caching examples"""
    print(f"{'='*80}")
    print(f"Prefix Caching: 50-80% Cost Reduction")
    print(f"{'='*80}\n")

    print(f"ðŸ’¡ Prefix caching reuses computation from repeated prompt prefixes")
    print(f"   Perfect for RAG, templates, and conversations\n")

    # Run scenarios
    metrics = []
    metrics.append(simulate_rag_with_caching())
    metrics.append(simulate_template_generation())
    metrics.append(simulate_multi_turn_conversation())

    # Code examples
    vllm_prefix_caching_code_example()
    production_deployment_pattern()
    roi_analysis()

    # Summary
    print(f"\n{'='*80}")
    print(f"âœ… Summary")
    print(f"{'='*80}\n")

    total_savings = sum(m.savings_pct for m in metrics) / len(metrics)
    total_speedup = sum(m.speedup for m in metrics) / len(metrics)

    print(f"Average savings: {total_savings:.1f}%")
    print(f"Average speedup: {total_speedup:.1f}x")
    print(f"\nðŸŽ¯ Best Use Cases:")
    print(f"  1. RAG with fixed document context: 80% savings")
    print(f"  2. Template-based generation: 70% savings")
    print(f"  3. Multi-turn conversations: 50% savings")
    print(f"\nðŸ“š Next Steps:")
    print(f"  1. Add ENABLE_PREFIX_CACHING=True to .env")
    print(f"  2. Structure prompts: fixed content first, variable last")
    print(f"  3. Deploy and enjoy 50-80% cost reduction!")
    print(f"\nðŸ’° At 10K requests/day:")
    print(f"  Without caching: $324/month")
    print(f"  With RAG caching: $65/month")
    print(f"  Yearly savings: $3,108")


if __name__ == "__main__":
    main()
